# The Main One

## The Basic Idea

$$L(\beta,D|Y)=\int \prod_{i=1}^{n} f_{y_i|u}(y_i|b_i,\beta)f_{b_i}(b_i|D)db_i$$

Notations :

$y$: Variable for the fixed effect 

$b$: Variable for the random effect 

$\beta$: Parameters for the fixed effect

$D$: Parameters for the random effect

The dimension of the integral is equal to the levels of the random factors (i.e., the number of observations).
 
## Model and R Code

Covarance Matrix for $n$ observations:

$$V=\sigma^2 \begin{bmatrix} 1 & \rho & \rho^2 & ... & \rho^{n-1} & \rho^n \\ \rho & 1 & \rho & ... & \rho^{n-2}& \rho^{n-1}\\ \rho^2 & \rho & 1 & ... & \rho^{n-3}& \rho^{n-2} \\ ...\\ \rho^n & \rho^{n-1} & \rho^{n-2} & ... & \rho & 1 \end{bmatrix}$$


The inverse matrix is as follows:

$$Q=V^{-1}=\frac{1}{\sigma^2(1-\rho)} \begin{bmatrix} 1 & -\rho & 0 & ... & 0 & 0 \\ -\rho & 1+\rho^2 & -\rho & ... & 0 & 0\\ 0 & -\rho & 1+\rho^2 & ... & 0 & 0 \\ ...\\ 0 & 0 & 0 & ... & 1+\rho^2 &-\rho\\ 0 & 0 & 0 & ... & -\rho & 1 \end{bmatrix}$$

$$N(-\sum_{j\neq k} Q_{kj}b_j^{(m)}Q_{kk}^{-1},Q_{kk}^{-1})$$
$$ln L(\beta, \theta; Y,b)=\ell=lnf_{Y|b}(Y|b,\beta)+lnf_b(b|\theta)$$

$$a^{(m+1)}=a^{(m)}+\tau(a^{(m)})^{-1} S(a^{(m)})$$
Where,

$$\tau(a) = -E(\frac{\partial^2 \ell}{\partial \alpha \partial \alpha^{'}}|Y)$$

$$S(a) = E(\frac{\partial \ell}{\partial \alpha }|Y)$$
Note that, $\alpha$ is a combination of two sets of parameters.

$$\alpha = \binom{\beta}{b} $$

$$\ell=\sum_{i=1}^{n}\{[y_i ln (\frac{e^{\beta^Tx_i+b_i}}{1+e^{\beta^Tx_i+b_i}}) + (1-y_i) ln(1-\frac{e^{\beta^Tx_i+b_i}}{1+e^{\beta^Tx_i+b_i}})]+lnf_b(b_i|\theta)\}$$


$$\frac{\partial lnf(Y|b, \beta)}{\partial \beta}=X^{'}(Y-E(Y|b))$$

$$\frac{\partial lnf(Y|b, \beta)}{\partial \beta \partial \beta^{'}}=-X^{'}(Y-E(Y|b))$$

$$\begin{aligned} \nabla \ell &= \sum_{i=1}^{n} [y_i \frac{1}{p(\beta ^T x_i+b_i)} \frac{\partial p(\beta ^T x_i+b_i)}{\partial (\beta ^T x_i+b_i)}\frac{\partial (\beta ^T x_i+b_i)}{\partial \beta}+(1-y_i) \frac{1}{1-p(\beta ^T x_i+b_i)}(-1)\frac{\partial p(\beta ^T x_i+b_i)}{\partial (\beta ^T x_i+b_i)}\frac{\partial (\beta ^T x_i+b_i)}{\partial \beta}] \\ &=\sum_{i=1}^{n} x_i^T[y_i-p(\beta ^T x_i+b_i)] \\ &= \sum_{i=1}^{n} x_i^T[y_i-\frac{e^{\beta^Tx_i+b_i}}{1+e^{\beta^Tx_i+b_i}}] \end{aligned}$$


The Newton Raphson algorithm needs the second order.

$$\begin{aligned} \nabla^2 \ell &=\frac{\partial \sum_{i=1}^{n} x_i^T[y_i-p(\beta ^T x_i+b_i)]}{\partial \beta} \\ &=-\sum_{i=1}^{n} x_i^T\frac{\partial p(\beta ^T x_i+b_i) }{\partial \beta}\\ &=-\sum_{i=1}^{n} x_i^T\frac{\partial p(\beta ^T x_i+b_i) }{\partial (\beta^Tx_i+b_i)} \frac{\partial (\beta^Tx_i+b_i)}{\partial \beta}\\ &=-\sum_{i=1}^{n} x_i^T p(\beta ^T x_i+b_i)(1-p(\beta ^T x_i+b_i))x_i \\ &=-\sum_{i=1}^{n} x_i^T \frac{e^{\beta^Tx_i+b_i}}{1+e^{\beta^Tx_i+b_i}}(1-\frac{e^{\beta^Tx_i+b_i}}{1+e^{\beta^Tx_i+b_i}})x_i \end{aligned}$$



```{R eval = FALSE}

#install.packages("CVTuningCov")
library(CVTuningCov) # Will be used to generate AR1 matrix

set.seed(123)
y<-c(1,1,1,0,0,1,1,0,1,0) ## observations

n=length(y) # the number of observations


#Establish the exp function
Expit<-function(x){exp(x)/(1+exp(x))}
#Y: observations
#b: random effect
#beta_0:fixed effect->intercept (or, mean of Y)

log_pdf_function<-function(Y,b,beta_0)
  {mean_prob<-Expit(beta_0+b)
  dbinom(Y,1,mean_prob,log = TRUE)
  }

b_records<-rep(0,n)  #Initial values for the random effect
rho_records<-0.5 #Initial value for rho
sigma_recoards<-2  #Initial value for sigma
mean_0<-0 # Initial mean value for normal distribution (of the random effect)
beta<-0.5 # Initial value for the intercept of Y


f_random<-function(sigma_recoards, rho_records,beta)
{
  
co_matrix<-(sigma_recoards^2)*AR1(n,rho_records) # covariance matrix
co_matrix_inverse<-solve(co_matrix)  # inverse covariance matrix


for (k in 1:n)
 {
  # Variance for the random effect
  sd_0<-1/(co_matrix_inverse[k,k])
  
  for(j in 1:n)
      { # Make sure that k is not equal to j, otherwise 0
        Q_kj<-ifelse(j!=k,co_matrix_inverse[k,j],0)
        # Calculate the mean for the random effect; sum of mean in a loop
        mean_0<-mean_0-(Q_kj/co_matrix_inverse[k,k])*b_records[j]
      }
  # Draw a random number from the normal distribution for the random effect
  b_candidate<-rnorm(1,mean_0,sd_0)
  
  current_lp<-log_pdf_function(y[k],b_records[k],beta)
  candidate_lp<-log_pdf_function(y[k],b_candidate,beta)
  
  Smaller_value<-min(exp(candidate_lp-current_lp),1)
  # Draw a random number from the uniform distribution
  Random_probability<-runif(1)
  # Update b (i.e., random variable)
  b_records[k]<-ifelse(Random_probability<Smaller_value,b_candidate,b_records[k])
}

return(b_records)
}

# Print result
#b_records<-f_random(1,0.8,0.3) 
```


In the following, I will try to add the random effect. 

```{R eval = FALSE}
x_intercept<-rep(1,n)
x_intercept<-as.matrix(x_intercept)

#We need to set random starting values.

tolerance=1e-3
max_its=2000;iteration=1;difference=2
W<-matrix(0,n,n)
beta_old<-0.4

while(difference>tolerance & iteration<max_its)
  {
  b_records<-f_random(2,0.9,beta_old)
  print("first")
  print(beta_old)
  print(b_records)
  # The first order
  f_firstorder<-t(x_intercept)%*%(y-Expit(x_intercept%*%beta_old+b_records))
  print(f_firstorder)
  # The second order
  diag(W) = Expit(x_intercept%*%beta_old+b_records)*(1-Expit(x_intercept%*%beta_old+b_records))
  
  f_secondorder<--t(x_intercept)%*%W%*%x_intercept
  
  # Calculate the beta_updated
  beta_updated=beta_old-(solve(f_secondorder)%*%f_firstorder)
  
  difference=max(abs(beta_updated-beta_old));
  
  iteration=iteration+1;
  
  beta_old=beta_updated
  }

beta_old


```

## glmmTMB package

```{r eval = FALSE}
# https://becarioprecario.bitbucket.io/inla-gitbook/ch-intro.html
#https://cran.r-project.org/web/packages/glmmTMB/vignettes/covstruct.html
#install.packages("glmmTMB")
library(glmmTMB)

times <- factor(1:n)
levels(times)
group <- factor(rep(1,n))
dat0 <- data.frame(y,times,group)

glmmTMB(y ~ ar1(times + 0 | group), data=dat0)

```
